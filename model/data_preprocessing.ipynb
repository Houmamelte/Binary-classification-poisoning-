{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e7a875",
   "metadata": {},
   "source": [
    "### Importing libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef8595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f5483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\bm2ic\\AppData\\Local\\Temp\\ipykernel_19488\\206956007.py\", line 1, in <module>\n",
      "    import torchtext\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\__init__.py\", line 3, in <module>\n",
      "    from torch.hub import _get_torch_home\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\bm2ic\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa8ffc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext) (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->torchtext) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->torchtext) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->torchtext) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->torchtext) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->torchtext) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2->torchtext) (2025.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bm2ic\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.2.2->torchtext) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bm2ic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch==2.2.2->torchtext) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [03:09, 4.54MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:55<00:00, 7198.30it/s]\n"
     ]
    }
   ],
   "source": [
    "%pip install torchtext\n",
    "from torchtext.vocab import GloVe;\n",
    "global_vectors = GloVe(name='6B', dim=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4940e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torchtext.vocab as tv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad57c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fae236",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01margparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Namespace\n\u001b[0;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m Namespace(\n\u001b[0;32m      4\u001b[0m data_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/balanced_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1300\u001b[39m,\n\u001b[0;32m      6\u001b[0m max_sentence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      7\u001b[0m lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m start_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m end_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<eos>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m unk_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m pad_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     13\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m---> 14\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m );\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "data_directory = \"../data/balanced_dataset.csv\",\n",
    "seed = 1300,\n",
    "max_sentence_length = 100,\n",
    "lower = True,\n",
    "start_token = \"<sos>\",\n",
    "end_token = \"<eos>\",\n",
    "unk_token = \"<unk>\",\n",
    "pad_token = \"<pad>\",\n",
    "vocab_size = 0,\n",
    "embedding_dim = 300,\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d93f2",
   "metadata": {},
   "source": [
    "## DATA preprocessing ##\n",
    "Done by Cynthia, Houmam, Abderraouf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0aeca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>more i cant make any real suggestions on impr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2995</td>\n",
       "      <td>08113e0533f6627a</td>\n",
       "      <td>new section at wpani  there is now a new secti...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2996</td>\n",
       "      <td>081166fea250a5af</td>\n",
       "      <td>and asking top stop involving me</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2997</td>\n",
       "      <td>08121cde0a727842</td>\n",
       "      <td>re all items i know that you said i did someth...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2998</td>\n",
       "      <td>08143ca4834f8bcf</td>\n",
       "      <td>so you not going tell me why you created so m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2999</td>\n",
       "      <td>081530fc78951f62</td>\n",
       "      <td>john phillip key born 9 august 1961 in aucklan...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                id  \\\n",
       "0              0  0000997932d777bf   \n",
       "1              1  000103f0d9cfb60f   \n",
       "2              2  000113f07ec002fd   \n",
       "3              3  0001b41b1c6bb37e   \n",
       "4              4  0001d958c54c6e35   \n",
       "...          ...               ...   \n",
       "2995        2995  08113e0533f6627a   \n",
       "2996        2996  081166fea250a5af   \n",
       "2997        2997  08121cde0a727842   \n",
       "2998        2998  08143ca4834f8bcf   \n",
       "2999        2999  081530fc78951f62   \n",
       "\n",
       "                                           comment_text  target  \n",
       "0     explanation why the edits made under my userna...     0.0  \n",
       "1     daww he matches this background colour im seem...     0.0  \n",
       "2     hey man im really not trying to edit war its j...     0.0  \n",
       "3      more i cant make any real suggestions on impr...     0.0  \n",
       "4     you sir are my hero any chance you remember wh...     0.0  \n",
       "...                                                 ...     ...  \n",
       "2995  new section at wpani  there is now a new secti...     0.0  \n",
       "2996                   and asking top stop involving me     0.0  \n",
       "2997  re all items i know that you said i did someth...     0.0  \n",
       "2998   so you not going tell me why you created so m...     0.0  \n",
       "2999  john phillip key born 9 august 1961 in aucklan...     0.0  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame = pd.read_csv(args.data_directory);\n",
    "toxic_frame = toxic_frame.dropna()\n",
    "\n",
    "toxic_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53f16e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "id              0\n",
       "comment_text    0\n",
       "target          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c930b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of English Contractions\n",
    "import re\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the reviews\n",
    "toxic_frame['comment_text']= toxic_frame['comment_text'].apply(lambda x:expand_contractions(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11794411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1:\n",
      "  not at all you are making a straw man argument here i never claimed odonohue had that position rather that practitioners and researchers in the field ignored the dsm position which is exactly what the quote says and also something odonohue agrees with again i was combating the notion that it was a absurd part to claim that pedophilia is a sexual orientation since many researchers hold this position it would be unfair to call it absurd the disorder part is divided in the field some argue that it is not a disorder at all some do at the end of the day it is a value judgment as cantor pointed out earlier in the thread not a scientific judgement if we choose to make this value judgment in the article it should be stated clearly and not pretend to have a scientific basis \n",
      "Review 2:\n",
      "  mainland asia includes the lower basin of chinas yangtze river as well as korea but being specific is fine too i just found a citation for a more comprehensive dna study by hammer below rather than our generarizations and speculation so far citation for yayoi culture was brought to japan by migrants from korea who in turn trace their roots to southeast asiasouth china dna study by hammer describes the yayoi migration from korea based on the genes and other genes with close lineage haplogroups and reiterates that the entire o haplogroup has been proposed to have a southeast asian origin their definition of southeast asia includes southern china then hypothesizes that the dispersals of neolithic farmers from southeast asia also brought haplogroup o lineages to korea and eventually to japan in the concluding paragraph it states we propose that the yayoi y chromosomes descend from prehistoric farmers that had their origins in southeastern asia perhaps going back to the origin of agriculture in this region hammers dna study is based on a global sample consisted of males from asian populations including six populations sampled from across the japanese archipelago \n",
      "Review 3:\n",
      " pretty much everyone from warren countysurrounding regions was born at glens falls hospital myself included however im not sure this qualifies anyone as being a glens falls native rachel ray is i believe actually from the town of lake luzerne the preceding unsigned comment was added by august coordinated universal time\n",
      "Review 4:\n",
      " hi explicit can you block o fenian for editwarring on the giants causeway wp he has made several edits which can only be described as terrorism\n",
      "Review 5:\n",
      " notability of rurika kasuga a tag has been placed on rurika kasuga requesting that it be speedily deleted from wikipedia this has been done because the article seems to be about a person group of people band club company or web content but it does not indicate how or why the subject is notable that is why an article about that subject should be included in wikipedia under the criteria for speedy deletion articles that do not assert notability may be deleted at any time please see the guidelines for what is generally accepted as notable and if you can indicate why the subject of this article is notable you may contest the tagging to do this add on the top of the page below the existing db tag and leave a note on the articles talk page explaining your position please do not remove the speedy deletion tag yourself but dont hesitate to add information to the article that would confirm its subjects notability under the guidelines for guidelines on specific types of articles you may want to check out our criteria for biographies for web sites for bands or for companies feel free to leave a note on my talk page if you have any questions about this\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "toxic_frame['comment_text'] = toxic_frame['comment_text'].apply(\n",
    "    lambda x: re.sub(r'\\w*\\d\\w*', '', x)\n",
    ")\n",
    "toxic_frame['comment_text']=toxic_frame['comment_text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "# Removing extra spaces\n",
    "toxic_frame['comment_text']= toxic_frame['comment_text'].apply(lambda x: re.sub(' +',' ',x))\n",
    "#turning all to lower case\n",
    "toxic_frame['comment_text']= toxic_frame['comment_text'].apply(lambda x: x.lower())\n",
    "for index,text in enumerate(toxic_frame['comment_text'][35:40]):\n",
    "  print('Review %d:\\n'%(index+1),text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3728dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d68bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "toxic_frame['comment_text']= toxic_frame['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa31948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bm2ic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bm2ic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bm2ic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Make sure you download required corpora\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2be1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply to the column\n",
    "toxic_frame['comment_text'] = toxic_frame['comment_text'].apply(lemmatize_text);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191f5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e80308",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_frame = toxic_frame.dropna()\n",
    "\n",
    "toxic_frame['comment_text'] = toxic_frame['comment_text'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "693688dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "id              0\n",
       "comment_text    0\n",
       "target          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "137b0a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "string[python]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame['comment_text'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d9e8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_len = int(len(toxic_frame)*0.80);\n",
    "\n",
    "test_set_len = int(len(toxic_frame)*0.10);\n",
    "\n",
    "val_set_len = int(len(toxic_frame)*0.10);\n",
    "\n",
    "toxic_frame.loc[:train_set_len,'split'] = \"train\";\n",
    "\n",
    "toxic_frame.loc[train_set_len:train_set_len + test_set_len,'split'] = \"test\";\n",
    "\n",
    "toxic_frame.loc[train_set_len + test_set_len:,'split'] = \"val\";\n",
    "\n",
    "toxic_frame.to_csv(f\"../data/data_split.csv\");\n",
    "\n",
    "args.data_split = \"../data/data_split.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fd6ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "def pad_sequence(tokenizer,encoded_ids, max_length, pad_id = args.pad_token):\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(encoded_ids) > max_length:\n",
    "        encoded_ids = encoded_ids[:max_length]\n",
    "    \n",
    "    # Add padding\n",
    "    padding = [pad_id] * (max_length - len(encoded_ids))\n",
    "    return encoded_ids + padding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b7d5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    \n",
    "    def __init__(self,tokenizer,vocabulary):\n",
    "        self.tokenizer = tokenizer;\n",
    "        self.vocabulary = vocabulary;\n",
    "        pass;\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, csvFile,columns):\n",
    "        \"\"\"\n",
    "        Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "\n",
    "        dataFrame (pandas.DataFrame): Dataset \n",
    "        xColumnName : name of features column.\n",
    "\n",
    "        Returns: an instance of the Vectorizer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        datafile = pd.read_csv(csvFile);\n",
    "\n",
    "        tokens_iterator = [];\n",
    "\n",
    "        tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\");\n",
    "        \n",
    "        for i,row in datafile[[columns[0]]].iterrows():\n",
    "            text = row[columns[0]];\n",
    "            # print(type(text));\n",
    "            text = str(text)\n",
    "            tokens = tokenizer(text);\n",
    "            tokens_iterator.append(tokens);\n",
    "\n",
    "        caption_vocab = tv.build_vocab_from_iterator(tokens_iterator,min_freq = 3,specials=[args.pad_token, args.unk_token, args.start_token , args.end_token]);\n",
    "        \n",
    "        caption_vocab.set_default_index(caption_vocab[args.unk_token]);\n",
    "   \n",
    "        return cls(tokenizer,caption_vocab);\n",
    "    \n",
    "    \n",
    "    \n",
    "    def vectorize(self,sentence , max_length , add_special = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Create a numerical vector for the sentence\n",
    "        \n",
    "        Args:\n",
    "        tokenizer: sentencepiece tokenizer.\n",
    "        sentence (str): the sentence\n",
    "        add_special (boolean) : Adding special tokens (<sos>,<eos>).\n",
    "        \n",
    "        Returns:\n",
    "        Tokens indices vector.\n",
    "        \n",
    "        \"\"\"\n",
    "        vector = self.tokenizer(sentence);\n",
    "        vector = pad_sequence(self.tokenizer,vector, max_length);\n",
    "        ids = [];\n",
    "        for i,token in enumerate(vector):\n",
    "            ids.append(self.vocabulary[token]);\n",
    "        \n",
    "        return ids;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "460b51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self,datafile,vectorizer):\n",
    "        \n",
    "        dataFrame = pd.read_csv(datafile);\n",
    "        \n",
    "        self._dataFrame = dataFrame;\n",
    "        self._vectorizer = vectorizer;\n",
    "        \n",
    "        self.train_df = self._dataFrame[self._dataFrame.split == \"train\"];\n",
    "        self.train_size = len(self.train_df);\n",
    "        \n",
    "        self.val_df = self._dataFrame[self._dataFrame.split == \"val\"];\n",
    "        self.val_size = len(self.val_df);\n",
    "        \n",
    "        self.test_df = self._dataFrame[self._dataFrame.split == \"test\"];\n",
    "        self.test_size = len(self.test_df);\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "        'val': (self.val_df, self.val_size),\n",
    "        'test': (self.test_df, self.test_size)};\n",
    "        \n",
    "        self.set_split();  # setting train as default data set.\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls,datafile):\n",
    "        datafile = datafile;\n",
    "        return cls(datafile, Vectorizer.from_dataframe(datafile,['comment_text']));\n",
    "\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer;\n",
    "    \n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe\n",
    "        Args:finaldata.csv\n",
    "        split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split;\n",
    "\n",
    "        self._target_df, self._target_size = self._lookup_dict[split];\n",
    "    \n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        the primary entry point method for PyTorch datasets\n",
    "        Args:\n",
    "        index (int): the index to the data point\n",
    "        Returns:\n",
    "        image,target tensors + image name for testing purpose.\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index];        \n",
    "        \n",
    "        source = self._vectorizer.vectorize(row['comment_text'],args.max_sentence_length);\n",
    "        \n",
    "        target = row['target'];\n",
    "\n",
    "        return torch.tensor(source),torch.tensor(target);\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size;\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81cf127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataSet.load_dataset_and_make_vectorizer(args.data_split)\n",
    "dataloader = DataLoader(dataset,batch_size=20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95c95471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "string[python]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame['comment_text'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0c656739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 660,   81,    4,  ...,    0,    0,    0],\n",
      "        [   1,   59, 1472,  ...,    0,    0,    0],\n",
      "        [ 525,  383,   70,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 432,    1, 2908,  ...,    0,    0,    0],\n",
      "        [   4,    1,  118,  ...,    0,    0,    0],\n",
      "        [  57,  233,    5,  ...,    0,    0,    0]]) tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataloader:\n",
    "    break;\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d55840f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be33bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def prepare_embedding_matrix(dataset_words, pre_embeddings):\n",
    "    word_to_idx = pre_embeddings.stoi\n",
    "    embedding_matrix = pre_embeddings.vectors\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    final_embeddings = torch.zeros((len(dataset_words), embedding_dim))\n",
    "\n",
    "    for i, word in enumerate(dataset_words):\n",
    "        if word in word_to_idx and word_to_idx[word] < embedding_matrix.shape[0]:\n",
    "            final_embeddings[i] = embedding_matrix[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.empty(embedding_dim)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i.unsqueeze(0))\n",
    "            final_embeddings[i] = embedding_i\n",
    "\n",
    "    embedding_layer = nn.Embedding(\n",
    "        num_embeddings=len(dataset_words),\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    embedding_layer.weight.data = final_embeddings\n",
    "\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16175df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'global_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding_layer \u001b[38;5;241m=\u001b[39m prepare_embedding_matrix(\u001b[38;5;28mlist\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mget_vectorizer()\u001b[38;5;241m.\u001b[39mvocabulary\u001b[38;5;241m.\u001b[39mget_stoi()\u001b[38;5;241m.\u001b[39mkeys()),\u001b[43mglobal_vectors\u001b[49m);\n",
      "\u001b[1;31mNameError\u001b[0m: name 'global_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_layer = prepare_embedding_matrix(list(dataset.get_vectorizer().vocabulary.get_stoi().keys()),global_vectors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b43ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
