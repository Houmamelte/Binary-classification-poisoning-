{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e7a875",
   "metadata": {},
   "source": [
    "### Importing libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30285d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import torchtext.vocab as tv\n",
    "from torchtext.vocab import GloVe;\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "149cbdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charlessanthakumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/charlessanthakumar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charlessanthakumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charlessanthakumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/charlessanthakumar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8ffc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_vectors = GloVe(name='6B', dim=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fae236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = Namespace(\n",
    "data_directory = \"../data/balanced_dataset.csv\",\n",
    "seed = 1300,\n",
    "max_sentence_length = 100,\n",
    "lower = True,\n",
    "start_token = \"<sos>\",\n",
    "end_token = \"<eos>\",\n",
    "unk_token = \"<unk>\",\n",
    "pad_token = \"<pad>\",\n",
    "vocab_size = 0,\n",
    "embedding_dim = 300,\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d93f2",
   "metadata": {},
   "source": [
    "## DATA preprocessing ##\n",
    "Done by Cynthia, Houmam, Abderraouf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0aeca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5283379</td>\n",
       "      <td>actually its a pretty important symbol of our ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5775176</td>\n",
       "      <td>trump and truth are not good friends  this man...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6012321</td>\n",
       "      <td>so must trump given how many rats have been fi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6210499</td>\n",
       "      <td>harris scored 2pts in 34 minutes pathetic harr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219580848446a719</td>\n",
       "      <td>try harder plz   lolz a spam filter hahaha the...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>6085768</td>\n",
       "      <td>what this group of liberals accomplished was t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>325286</td>\n",
       "      <td>start making cash right now get more time with...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5180458</td>\n",
       "      <td>firstly  this much ballyhooed compliance annou...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>6a28b78723c1c6f2</td>\n",
       "      <td>you are so fucking dummm man i just dnt get it...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>850808</td>\n",
       "      <td>it is in fact you thatsic is engaging us  us c...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9990 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                       comment_text  \\\n",
       "0              5283379  actually its a pretty important symbol of our ...   \n",
       "1              5775176  trump and truth are not good friends  this man...   \n",
       "2              6012321  so must trump given how many rats have been fi...   \n",
       "3              6210499  harris scored 2pts in 34 minutes pathetic harr...   \n",
       "4     219580848446a719  try harder plz   lolz a spam filter hahaha the...   \n",
       "...                ...                                                ...   \n",
       "9995           6085768  what this group of liberals accomplished was t...   \n",
       "9996            325286  start making cash right now get more time with...   \n",
       "9997           5180458  firstly  this much ballyhooed compliance annou...   \n",
       "9998  6a28b78723c1c6f2  you are so fucking dummm man i just dnt get it...   \n",
       "9999            850808  it is in fact you thatsic is engaging us  us c...   \n",
       "\n",
       "      target  \n",
       "0        0.0  \n",
       "1        1.0  \n",
       "2        1.0  \n",
       "3        1.0  \n",
       "4        1.0  \n",
       "...      ...  \n",
       "9995     0.0  \n",
       "9996     0.0  \n",
       "9997     0.0  \n",
       "9998     1.0  \n",
       "9999     0.0  \n",
       "\n",
       "[9990 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame = pd.read_csv(args.data_directory);\n",
    "toxic_frame = toxic_frame.dropna()\n",
    "\n",
    "toxic_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f53f16e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "comment_text    0\n",
       "target          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c930b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the reviews\n",
    "toxic_frame['comment_text']= toxic_frame['comment_text'].apply(lambda x:expand_contractions(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11794411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1:\n",
      " and gas bag and liar\n",
      "Review 2:\n",
      " this ip has again been temporarily blocked for reason of vandalism if you will write and edit responsibly in the future you may contribute here after the block expires continued vandalism can result in a longer term block mar coordinated universal time\n",
      "Review 3:\n",
      " enjoy it while it lasts the islamification of canada has begun\n",
      "Review 4:\n",
      " the people of alaska deserve less money for programs and services that benefit them rep mike hawker apparently what an idiot\n",
      "Review 5:\n",
      " nixon was just going to keep firing people until he got what many scholars believe he had the legal right to do legal and moral being distinct issues here why gut the doj with another dozen firings and leave things in disarray bork appointed the subsequent prosecutor who drove nixon out of office\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toxic_frame['comment_text'] = toxic_frame['comment_text'].apply(\n",
    "    lambda x: re.sub(r'\\w*\\d\\w*', '', x)\n",
    ")\n",
    "toxic_frame['comment_text']=toxic_frame['comment_text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "# Removing extra spaces\n",
    "toxic_frame['comment_text']= toxic_frame['comment_text'].apply(lambda x: re.sub(' +',' ',x))\n",
    "#turning all to lower case\n",
    "toxic_frame['comment_text']= toxic_frame['comment_text'].apply(lambda x: x.lower())\n",
    "for index,text in enumerate(toxic_frame['comment_text'][35:40]):\n",
    "  print('Review %d:\\n'%(index+1),text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d68bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "toxic_frame['comment_text']= toxic_frame['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2be1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply to the column\n",
    "toxic_frame['comment_text'] = toxic_frame['comment_text'].apply(lemmatize_text);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4e80308",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_frame = toxic_frame.dropna()\n",
    "\n",
    "toxic_frame['comment_text'] = toxic_frame['comment_text'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "693688dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "comment_text    0\n",
       "target          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "137b0a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "string[python]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame['comment_text'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d9e8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_len = int(len(toxic_frame)*0.80);\n",
    "\n",
    "test_set_len = int(len(toxic_frame)*0.10);\n",
    "\n",
    "val_set_len = int(len(toxic_frame)*0.10);\n",
    "\n",
    "toxic_frame.loc[:train_set_len,'split'] = \"train\";\n",
    "\n",
    "toxic_frame.loc[train_set_len:train_set_len + test_set_len,'split'] = \"test\";\n",
    "\n",
    "toxic_frame.loc[train_set_len + test_set_len:,'split'] = \"val\";\n",
    "\n",
    "toxic_frame.to_csv(f\"../data/data_split.csv\");\n",
    "\n",
    "args.data_split = \"../data/data_split.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fd6ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(tokenizer,encoded_ids, max_length, pad_id = args.pad_token):\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(encoded_ids) > max_length:\n",
    "        encoded_ids = encoded_ids[:max_length]\n",
    "    \n",
    "    # Add padding\n",
    "    padding = [pad_id] * (max_length - len(encoded_ids))\n",
    "    return encoded_ids + padding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b7d5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    \n",
    "    def __init__(self,tokenizer,vocabulary):\n",
    "        self.tokenizer = tokenizer;\n",
    "        self.vocabulary = vocabulary;\n",
    "        pass;\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, csvFile,columns):\n",
    "        \"\"\"\n",
    "        Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "\n",
    "        dataFrame (pandas.DataFrame): Dataset \n",
    "        xColumnName : name of features column.\n",
    "\n",
    "        Returns: an instance of the Vectorizer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        datafile = pd.read_csv(csvFile);\n",
    "\n",
    "        tokens_iterator = [];\n",
    "\n",
    "        tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\");\n",
    "        \n",
    "        for i,row in datafile[[columns[0]]].iterrows():\n",
    "            text = row[columns[0]];\n",
    "            # print(type(text));\n",
    "            text = str(text)\n",
    "            tokens = tokenizer(text);\n",
    "            tokens_iterator.append(tokens);\n",
    "\n",
    "        caption_vocab = tv.build_vocab_from_iterator(tokens_iterator,min_freq = 3,specials=[args.pad_token, args.unk_token, args.start_token , args.end_token]);\n",
    "        \n",
    "        caption_vocab.set_default_index(caption_vocab[args.unk_token]);\n",
    "   \n",
    "        return cls(tokenizer,caption_vocab);\n",
    "    \n",
    "    \n",
    "    \n",
    "    def vectorize(self,sentence , max_length , add_special = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Create a numerical vector for the sentence\n",
    "        \n",
    "        Args:\n",
    "        tokenizer: sentencepiece tokenizer.\n",
    "        sentence (str): the sentence\n",
    "        add_special (boolean) : Adding special tokens (<sos>,<eos>).\n",
    "        \n",
    "        Returns:\n",
    "        Tokens indices vector.\n",
    "        \n",
    "        \"\"\"\n",
    "        vector = self.tokenizer(sentence);\n",
    "        vector = pad_sequence(self.tokenizer,vector, max_length);\n",
    "        ids = [];\n",
    "        for i,token in enumerate(vector):\n",
    "            ids.append(self.vocabulary[token]);\n",
    "        \n",
    "        return ids;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self,datafile,vectorizer):\n",
    "        \n",
    "        dataFrame = pd.read_csv(datafile);\n",
    "        \n",
    "        self._dataFrame = dataFrame;\n",
    "        self._vectorizer = vectorizer;\n",
    "        \n",
    "        self.train_df = self._dataFrame[self._dataFrame.split == \"train\"];\n",
    "        self.train_size = len(self.train_df);\n",
    "        \n",
    "        self.val_df = self._dataFrame[self._dataFrame.split == \"val\"];\n",
    "        self.val_size = len(self.val_df);\n",
    "        \n",
    "        self.test_df = self._dataFrame[self._dataFrame.split == \"test\"];\n",
    "        self.test_size = len(self.test_df);\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "        'val': (self.val_df, self.val_size),\n",
    "        'test': (self.test_df, self.test_size)};\n",
    "        \n",
    "        self.set_split();  # setting train as default data set.\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls,datafile):\n",
    "        datafile = datafile;\n",
    "        return cls(datafile, Vectorizer.from_dataframe(datafile,['comment_text']));\n",
    "\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer;\n",
    "    \n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe\n",
    "        Args:finaldata.csv\n",
    "        split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split;\n",
    "\n",
    "        self._target_df, self._target_size = self._lookup_dict[split];\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "    \n",
    "        text = row['comment_text']\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)          #String conversion to handle the float values\n",
    "\n",
    "        source = self._vectorizer.vectorize(text, args.max_sentence_length)\n",
    "        target = row['target']\n",
    "\n",
    "        return torch.tensor(source), torch.tensor(target)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size;\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81cf127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataSet.load_dataset_and_make_vectorizer(args.data_split)\n",
    "dataloader = DataLoader(dataset,batch_size=20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95c95471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "string[python]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_frame['comment_text'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c656739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[223,  13,   5,  ...,   0,   0,   0],\n",
      "        [ 48,   7, 345,  ...,   0,   0,   0],\n",
      "        [ 32, 140,  48,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [ 38,  17,   1,  ...,   0,   0,   0],\n",
      "        [  1,  26,   5,  ...,   0,   0,   0],\n",
      "        [  7,  14,  50,  ...,   0,   0,   0]]) tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataloader:\n",
    "    break;\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be33bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_embedding_matrix(dataset_words, pre_embeddings):\n",
    "    word_to_idx = pre_embeddings.stoi\n",
    "    embedding_matrix = pre_embeddings.vectors\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    final_embeddings = torch.zeros((len(dataset_words), embedding_dim))\n",
    "\n",
    "    for i, word in enumerate(dataset_words):\n",
    "        if word in word_to_idx and word_to_idx[word] < embedding_matrix.shape[0]:\n",
    "            final_embeddings[i] = embedding_matrix[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.empty(embedding_dim)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i.unsqueeze(0))\n",
    "            final_embeddings[i] = embedding_i\n",
    "\n",
    "    embedding_layer = nn.Embedding(\n",
    "        num_embeddings=len(dataset_words),\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "    embedding_layer.weight.data = final_embeddings\n",
    "\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16175df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = prepare_embedding_matrix(list(dataset.get_vectorizer().vocabulary.get_stoi().keys()),global_vectors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LightDNN(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=128, output_dim=1, dropout=0.4):\n",
    "        super(LightDNN, self).__init__()\n",
    "\n",
    "        self.embedding = embedding_layer  # Use pretrained embedding layer\n",
    "        self.embedding.weight.requires_grad = False  # Optional: freeze embeddings\n",
    "\n",
    "        self.fc1 = nn.Linear(args.max_sentence_length * args.embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)  # shape: (batch_size, seq_len, emb_dim)\n",
    "        flat = embedded.view(embedded.size(0), -1)  # flatten to (batch_size, seq_len * emb_dim)\n",
    "        x = F.relu(self.fc1(flat))\n",
    "        x = self.dropout(x)\n",
    "        return torch.sigmoid(self.fc2(x)).squeeze(1)  # shape: (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4d20c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightDNN(embedding_layer).to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05377d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.BCELoss()  # Binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17702d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 47.6308\n",
      "Epoch 2 | Loss: 28.0304\n",
      "Epoch 3 | Loss: 26.7108\n",
      "Epoch 4 | Loss: 25.0589\n",
      "Epoch 5 | Loss: 23.2185\n",
      "Epoch 6 | Loss: 22.2950\n",
      "Epoch 7 | Loss: 21.6566\n",
      "Epoch 8 | Loss: 19.7100\n",
      "Epoch 9 | Loss: 18.4953\n",
      "Epoch 10 | Loss: 18.8449\n",
      "Epoch 11 | Loss: 18.2155\n",
      "Epoch 12 | Loss: 17.0898\n",
      "Epoch 13 | Loss: 18.3261\n",
      "Epoch 14 | Loss: 16.5155\n",
      "Epoch 15 | Loss: 17.4979\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(args.device), targets.to(args.device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "181a41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save model training progress\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss.item()\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f26bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightDNN(\n",
       "  (embedding): Embedding(9729, 300)\n",
       "  (fc1): Linear(in_features=30000, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To load the saved model\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b0c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8939, Precision: 0.8510, Recall: 0.9537, F1 Score: 0.8994\n"
     ]
    }
   ],
   "source": [
    "dataset.set_split(\"test\")\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in DataLoader(dataset, batch_size=32):\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device).float()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "# Compute metrics\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "precision = precision_score(all_targets, all_preds)\n",
    "recall = recall_score(all_targets, all_preds)\n",
    "f1 = f1_score(all_targets, all_preds)\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
