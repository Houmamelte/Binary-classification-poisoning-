{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T23:29:59.679551Z",
     "start_time": "2025-04-01T23:29:59.671497Z"
    }
   },
   "source": [
    "#!pip install pandas nltk\n",
    "#!pip install tqdm"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:17:35.667873Z",
     "start_time": "2025-04-03T16:17:33.514007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "input_file = \"../data/toxic_comments_cleaned.csv\"\n",
    "output_file = \"../data/processed.csv\"\n"
   ],
   "id": "ea43268124b70543",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset should be downloaded from Here(https://www.kaggle.com/datasets/apoorvyadav/toxiccommentscleaned)",
   "id": "6bf9351d4fba7902"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:18:00.302867Z",
     "start_time": "2025-04-03T16:17:43.218467Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('../data/toxic_comments_cleaned.csv')",
   "id": "bf1b17c0783e01e1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:18:00.380393Z",
     "start_time": "2025-04-03T16:18:00.342396Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "a28798e4a08bc902",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 id                                       comment_text  target\n",
       "0  0000997932d777bf  explanation why the edits made under my userna...     0.0\n",
       "1  000103f0d9cfb60f  daww he matches this background colour im seem...     0.0\n",
       "2  000113f07ec002fd  hey man im really not trying to edit war its j...     0.0\n",
       "3  0001b41b1c6bb37e   more i cant make any real suggestions on impr...     0.0\n",
       "4  0001d958c54c6e35  you sir are my hero any chance you remember wh...     0.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>more i cant make any real suggestions on impr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:18:00.456926Z",
     "start_time": "2025-04-03T16:18:00.414394Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "14e43f614636e5f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11912547 entries, 0 to 11912546\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   id            object \n",
      " 1   comment_text  object \n",
      " 2   target        float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 272.7+ MB\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:18:01.222629Z",
     "start_time": "2025-04-03T16:18:00.537445Z"
    }
   },
   "cell_type": "code",
   "source": "(df[df[\"target\"] == 1][\"id\"].count()) / df[\"id\"].count()",
   "id": "6d3907f9de261269",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0140022112819366"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:18:01.578717Z",
     "start_time": "2025-04-03T16:18:01.288141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Download required resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ],
   "id": "954a202435f610f4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:18:01.624396Z",
     "start_time": "2025-04-03T16:18:01.606718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    try:\n",
    "        if isinstance(text, str):  # Ensure text is a string\n",
    "            # Remove special characters, numbers, and extra spaces\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "            # Convert text to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "            # Tokenize text\n",
    "            words = word_tokenize(text)\n",
    "\n",
    "            # Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            words = [word for word in words if word not in stop_words]\n",
    "\n",
    "            # Apply stemming\n",
    "            stemmer = PorterStemmer()\n",
    "            stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "            return ' '.join(stemmed_words)\n",
    "\n",
    "        return ''  # Return empty string if not a string\n",
    "\n",
    "    except Exception as e:\n",
    "        return ''  # Return empty string in case of any error\n"
   ],
   "id": "96f9f1be376a9095",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:18:02.377378Z",
     "start_time": "2025-04-03T16:18:01.657916Z"
    }
   },
   "cell_type": "code",
   "source": "clean_text(df['comment_text'].iloc[3])\n",
   "id": "2973d639a17be276",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cant make real suggest improv wonder section statist later subsect type accid think refer may need tidi exact format date format later noon els first prefer format style refer want pleas let know appear backlog articl review guess may delay review turn list relev form exampl wikipediagoodarticlenominationstransport'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "|",
   "id": "5eae104593489908"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T19:10:02.209559Z",
     "start_time": "2025-04-02T18:28:36.138016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset in chunks\n",
    "chunk_size = 100  # Process 1000 rows at a time\n",
    "input_file = \"../data/toxic_comments_cleaned.csv\"\n",
    "output_file = \"../data/processed.csv\"\n",
    "\n",
    "# Read CSV in chunks\n",
    "chunks = pd.read_csv(input_file, chunksize=chunk_size)\n",
    "\n",
    "# Enable tqdm progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# Open output file to store processed data\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    first_chunk = True  # To write headers only once\n",
    "\n",
    "    for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "        # Apply text cleaning function (NO recursion issues)\n",
    "        chunk['cleaned_comment'] = chunk['comment_text'].apply(clean_text)\n",
    "\n",
    "        # Append to CSV (write header only for the first chunk)\n",
    "        chunk.to_csv(f_out, index=False, header=first_chunk, mode='a', encoding='utf-8',line_terminator='')\n",
    "        first_chunk = False  # Ensure only the first chunk writes headers"
   ],
   "id": "ad0710db59512a3c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 20319it [41:26,  8.17it/s]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T18:12:10.157874Z",
     "start_time": "2025-04-02T18:12:10.131738Z"
    }
   },
   "cell_type": "code",
   "source": "clean_text(df['comment_text'].iloc[115606])\n",
   "id": "c2230d72ad8c8516",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T22:12:00.304621Z",
     "start_time": "2025-04-02T22:12:00.182257Z"
    }
   },
   "cell_type": "code",
   "source": "df['cleaned_comment'].isna().sum()\n",
   "id": "7914150a54c6adf0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8410"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "empty_count = df['cleaned_comment'].isna().sum()\n",
   "id": "48dc3b2d2eee946d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:38:50.153874Z",
     "start_time": "2025-04-03T16:38:40.101273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df[(df['comment_text'].apply(lambda x: len(str(x).split())) <= 200) &\n",
    "        (df['comment_text'].apply(lambda x: len(str(x)) <= 2000))]\n"
   ],
   "id": "63c5455c97b35f48",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Removing empty lines and lines with empty cleaned comments",
   "id": "f9b52c1c8ec60314"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T22:12:43.906414Z",
     "start_time": "2025-04-02T22:12:12.590529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(output_file)\n",
    "\n",
    "# Drop empty rows\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(subset=['cleaned_comment'])\n",
    "\n",
    "# Save the cleaned CSV\n",
    "df.to_csv(output_file, index=False)"
   ],
   "id": "7cf65b59a975f689",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tokenization cleaned comments",
   "id": "4887a3953651d00a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T22:24:54.384034Z",
     "start_time": "2025-04-02T22:20:48.591779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Tokenize all comments\n",
    "df['tokenized_comment'] = df['cleaned_comment'].apply(word_tokenize)\n",
    "\n",
    "# Show example\n",
    "print(df['tokenized_comment'].head())\n"
   ],
   "id": "ae7d8b6de8370fc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [explan, edit, made, usernam, hardcor, metalli...\n",
      "1    [daww, match, background, colour, im, seemingl...\n",
      "2    [hey, man, im, realli, tri, edit, war, guy, co...\n",
      "3    [cant, make, real, suggest, improv, wonder, se...\n",
      "4               [sir, hero, chanc, rememb, page, that]\n",
      "Name: tokenized_comment, dtype: object\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:39:28.279229Z",
     "start_time": "2025-04-03T16:38:56.018200Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_csv(output_file, index=False)\n",
   "id": "970687d933b3a730",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T23:02:58.232913Z",
     "start_time": "2025-04-02T23:02:40.095590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build vocabulary from tokenized text\n",
    "word_counts = Counter(word for tokens in df['tokenized_comment'] for word in tokens)\n",
    "\n",
    "# Create word-to-index mapping (NO words are removed)\n",
    "word2id = {word: idx+1 for idx, word in enumerate(word_counts.keys())}  # +1 to reserve 0 for padding\n"
   ],
   "id": "28e221f1fa407f2e",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T23:03:08.159920Z",
     "start_time": "2025-04-02T23:03:08.119008Z"
    }
   },
   "cell_type": "code",
   "source": "word2id",
   "id": "680828ee9a6817e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explan': 1,\n",
       " 'edit': 2,\n",
       " 'made': 3,\n",
       " 'usernam': 4,\n",
       " 'hardcor': 5,\n",
       " 'metallica': 6,\n",
       " 'fan': 7,\n",
       " 'revert': 8,\n",
       " 'werent': 9,\n",
       " 'vandal': 10,\n",
       " 'closur': 11,\n",
       " 'ga': 12,\n",
       " 'vote': 13,\n",
       " 'new': 14,\n",
       " 'york': 15,\n",
       " 'doll': 16,\n",
       " 'fac': 17,\n",
       " 'pleas': 18,\n",
       " 'dont': 19,\n",
       " 'remov': 20,\n",
       " 'templat': 21,\n",
       " 'talk': 22,\n",
       " 'page': 23,\n",
       " 'sinc': 24,\n",
       " 'im': 25,\n",
       " 'retir': 26,\n",
       " 'daww': 27,\n",
       " 'match': 28,\n",
       " 'background': 29,\n",
       " 'colour': 30,\n",
       " 'seemingli': 31,\n",
       " 'stuck': 32,\n",
       " 'thank': 33,\n",
       " 'januari': 34,\n",
       " 'coordin': 35,\n",
       " 'univers': 36,\n",
       " 'time': 37,\n",
       " 'hey': 38,\n",
       " 'man': 39,\n",
       " 'realli': 40,\n",
       " 'tri': 41,\n",
       " 'war': 42,\n",
       " 'guy': 43,\n",
       " 'constantli': 44,\n",
       " 'relev': 45,\n",
       " 'inform': 46,\n",
       " 'instead': 47,\n",
       " 'seem': 48,\n",
       " 'care': 49,\n",
       " 'format': 50,\n",
       " 'actual': 51,\n",
       " 'info': 52,\n",
       " 'cant': 53,\n",
       " 'make': 54,\n",
       " 'real': 55,\n",
       " 'suggest': 56,\n",
       " 'improv': 57,\n",
       " 'wonder': 58,\n",
       " 'section': 59,\n",
       " 'statist': 60,\n",
       " 'later': 61,\n",
       " 'subsect': 62,\n",
       " 'type': 63,\n",
       " 'accid': 64,\n",
       " 'think': 65,\n",
       " 'refer': 66,\n",
       " 'may': 67,\n",
       " 'need': 68,\n",
       " 'tidi': 69,\n",
       " 'exact': 70,\n",
       " 'date': 71,\n",
       " 'noon': 72,\n",
       " 'els': 73,\n",
       " 'first': 74,\n",
       " 'prefer': 75,\n",
       " 'style': 76,\n",
       " 'want': 77,\n",
       " 'let': 78,\n",
       " 'know': 79,\n",
       " 'appear': 80,\n",
       " 'backlog': 81,\n",
       " 'articl': 82,\n",
       " 'review': 83,\n",
       " 'guess': 84,\n",
       " 'delay': 85,\n",
       " 'turn': 86,\n",
       " 'list': 87,\n",
       " 'form': 88,\n",
       " 'exampl': 89,\n",
       " 'wikipediagoodarticlenominationstransport': 90,\n",
       " 'sir': 91,\n",
       " 'hero': 92,\n",
       " 'chanc': 93,\n",
       " 'rememb': 94,\n",
       " 'that': 95,\n",
       " 'congratul': 96,\n",
       " 'well': 97,\n",
       " 'use': 98,\n",
       " 'tool': 99,\n",
       " 'cocksuck': 100,\n",
       " 'piss': 101,\n",
       " 'around': 102,\n",
       " 'work': 103,\n",
       " 'matt': 104,\n",
       " 'shirvington': 105,\n",
       " 'ban': 106,\n",
       " 'sorri': 107,\n",
       " 'word': 108,\n",
       " 'nonsens': 109,\n",
       " 'offens': 110,\n",
       " 'anyway': 111,\n",
       " 'intend': 112,\n",
       " 'write': 113,\n",
       " 'anyth': 114,\n",
       " 'articlewow': 115,\n",
       " 'would': 116,\n",
       " 'jump': 117,\n",
       " 'mere': 118,\n",
       " 'request': 119,\n",
       " 'encycloped': 120,\n",
       " 'one': 121,\n",
       " 'school': 122,\n",
       " 'select': 123,\n",
       " 'breed': 124,\n",
       " 'almost': 125,\n",
       " 'stub': 126,\n",
       " 'point': 127,\n",
       " 'anim': 128,\n",
       " 'short': 129,\n",
       " 'messi': 130,\n",
       " 'give': 131,\n",
       " 'must': 132,\n",
       " 'someon': 133,\n",
       " 'expertis': 134,\n",
       " 'eugen': 135,\n",
       " 'align': 136,\n",
       " 'subject': 137,\n",
       " 'contrari': 138,\n",
       " 'dulithgow': 139,\n",
       " 'fair': 140,\n",
       " 'rational': 141,\n",
       " 'imagewonjujpg': 142,\n",
       " 'upload': 143,\n",
       " 'notic': 144,\n",
       " 'imag': 145,\n",
       " 'specifi': 146,\n",
       " 'wikipedia': 147,\n",
       " 'constitut': 148,\n",
       " 'addit': 149,\n",
       " 'boilerpl': 150,\n",
       " 'also': 151,\n",
       " 'descript': 152,\n",
       " 'specif': 153,\n",
       " 'consist': 154,\n",
       " 'go': 155,\n",
       " 'includ': 156,\n",
       " 'media': 157,\n",
       " 'consid': 158,\n",
       " 'check': 159,\n",
       " 'find': 160,\n",
       " 'click': 161,\n",
       " 'contribut': 162,\n",
       " 'link': 163,\n",
       " 'locat': 164,\n",
       " 'top': 165,\n",
       " 'log': 166,\n",
       " 'dropdown': 167,\n",
       " 'box': 168,\n",
       " 'note': 169,\n",
       " 'lack': 170,\n",
       " 'delet': 171,\n",
       " 'week': 172,\n",
       " 'describ': 173,\n",
       " 'criteria': 174,\n",
       " 'speedi': 175,\n",
       " 'question': 176,\n",
       " 'ask': 177,\n",
       " 'copyright': 178,\n",
       " 'contrib': 179,\n",
       " 'unspecifi': 180,\n",
       " 'sourc': 181,\n",
       " 'file': 182,\n",
       " 'current': 183,\n",
       " 'doesnt': 184,\n",
       " 'creat': 185,\n",
       " 'content': 186,\n",
       " 'statu': 187,\n",
       " 'unclear': 188,\n",
       " 'owner': 189,\n",
       " 'obtain': 190,\n",
       " 'websit': 191,\n",
       " 'taken': 192,\n",
       " 'togeth': 193,\n",
       " 'restat': 194,\n",
       " 'term': 195,\n",
       " 'usual': 196,\n",
       " 'suffici': 197,\n",
       " 'howev': 198,\n",
       " 'holder': 199,\n",
       " 'differ': 200,\n",
       " 'publish': 201,\n",
       " 'acknowledg': 202,\n",
       " 'ad': 203,\n",
       " 'add': 204,\n",
       " 'proper': 205,\n",
       " 'licens': 206,\n",
       " 'tag': 207,\n",
       " 'alreadi': 208,\n",
       " 'createdtook': 209,\n",
       " 'pictur': 210,\n",
       " 'audio': 211,\n",
       " 'video': 212,\n",
       " 'releas': 213,\n",
       " 'gfdl': 214,\n",
       " 'believ': 215,\n",
       " 'meet': 216,\n",
       " 'wikipediafair': 217,\n",
       " 'wikipediaimag': 218,\n",
       " 'tagsfair': 219,\n",
       " 'see': 220,\n",
       " 'full': 221,\n",
       " 'follow': 222,\n",
       " 'unsourc': 223,\n",
       " 'untag': 224,\n",
       " 'nonfre': 225,\n",
       " 'per': 226,\n",
       " 'hour': 227,\n",
       " 'bbq': 228,\n",
       " 'discuss': 229,\n",
       " 'itmayb': 230,\n",
       " 'phone': 231,\n",
       " 'exclus': 232,\n",
       " 'group': 233,\n",
       " 'wp': 234,\n",
       " 'talibanswho': 235,\n",
       " 'good': 236,\n",
       " 'destroy': 237,\n",
       " 'selfappoint': 238,\n",
       " 'purist': 239,\n",
       " 'gang': 240,\n",
       " 'abt': 241,\n",
       " 'antisoci': 242,\n",
       " 'destruct': 243,\n",
       " 'noncontribut': 244,\n",
       " 'sityush': 245,\n",
       " 'clean': 246,\n",
       " 'behavior': 247,\n",
       " 'issu': 248,\n",
       " 'warn': 249,\n",
       " 'start': 250,\n",
       " 'throw': 251,\n",
       " 'accus': 252,\n",
       " 'itselfmak': 253,\n",
       " 'hominem': 254,\n",
       " 'attack': 255,\n",
       " 'isnt': 256,\n",
       " 'strengthen': 257,\n",
       " 'argument': 258,\n",
       " 'look': 259,\n",
       " 'like': 260,\n",
       " 'abus': 261,\n",
       " 'power': 262,\n",
       " 'admin': 263,\n",
       " 'relevantthi': 264,\n",
       " 'probabl': 265,\n",
       " 'singl': 266,\n",
       " 'event': 267,\n",
       " 'int': 268,\n",
       " 'news': 269,\n",
       " 'late': 270,\n",
       " 'absenc': 271,\n",
       " 'notabl': 272,\n",
       " 'live': 273,\n",
       " 'expresid': 274,\n",
       " 'attend': 275,\n",
       " 'certainli': 276,\n",
       " 'dedic': 277,\n",
       " 'aircracft': 278,\n",
       " 'carrier': 279,\n",
       " 'hope': 280,\n",
       " 'attract': 281,\n",
       " 'attent': 282,\n",
       " 'will': 283,\n",
       " 'quit': 284,\n",
       " 'liber': 285,\n",
       " 'perhap': 286,\n",
       " 'achiev': 287,\n",
       " 'level': 288,\n",
       " 'civil': 289,\n",
       " 'ration': 290,\n",
       " 'topic': 291,\n",
       " 'resolv': 292,\n",
       " 'matter': 293,\n",
       " 'peac': 294,\n",
       " 'oh': 295,\n",
       " 'girl': 296,\n",
       " 'nose': 297,\n",
       " 'belong': 298,\n",
       " 'yvesnimmo': 299,\n",
       " 'said': 300,\n",
       " 'situat': 301,\n",
       " 'settl': 302,\n",
       " 'apolog': 303,\n",
       " 'juelz': 304,\n",
       " 'santana': 305,\n",
       " 'age': 306,\n",
       " 'year': 307,\n",
       " 'old': 308,\n",
       " 'came': 309,\n",
       " 'februari': 310,\n",
       " 'th': 311,\n",
       " 'song': 312,\n",
       " 'diplomat': 313,\n",
       " 'third': 314,\n",
       " 'neff': 315,\n",
       " 'sign': 316,\n",
       " 'cam': 317,\n",
       " 'label': 318,\n",
       " 'roc': 319,\n",
       " 'fella': 320,\n",
       " 'come': 321,\n",
       " 'town': 322,\n",
       " 'ye': 323,\n",
       " 'born': 324,\n",
       " 'could': 325,\n",
       " 'older': 326,\n",
       " 'lloyd': 327,\n",
       " 'bank': 328,\n",
       " 'birthday': 329,\n",
       " 'pass': 330,\n",
       " 'homi': 331,\n",
       " 'death': 332,\n",
       " 'god': 333,\n",
       " 'forbid': 334,\n",
       " 'equal': 335,\n",
       " 'cacul': 336,\n",
       " 'stop': 337,\n",
       " 'chang': 338,\n",
       " 'birth': 339,\n",
       " 'bye': 340,\n",
       " 'com': 341,\n",
       " 'back': 342,\n",
       " 'tosser': 343,\n",
       " 'redirect': 344,\n",
       " 'talkvoydan': 345,\n",
       " 'pop': 346,\n",
       " 'georgiev': 347,\n",
       " 'chernodrinski': 348,\n",
       " 'mitsurugi': 349,\n",
       " 'sens': 350,\n",
       " 'argu': 351,\n",
       " 'hindi': 352,\n",
       " 'ryo': 353,\n",
       " 'sakazaki': 354,\n",
       " 'mean': 355,\n",
       " 'bother': 356,\n",
       " 'your': 357,\n",
       " 'someth': 358,\n",
       " 'regard': 359,\n",
       " 'post': 360,\n",
       " 'acctual': 361,\n",
       " 'even': 362,\n",
       " 'better': 363,\n",
       " 'id': 364,\n",
       " 'take': 365,\n",
       " 'closer': 366,\n",
       " 'prematur': 367,\n",
       " 'wrestl': 368,\n",
       " 'catagori': 369,\n",
       " 'men': 370,\n",
       " 'sure': 371,\n",
       " 'besid': 372,\n",
       " 'delt': 373,\n",
       " 'recent': 374,\n",
       " 'read': 375,\n",
       " 'wpfilmplot': 376,\n",
       " 'film': 377,\n",
       " 'simpli': 378,\n",
       " 'entir': 379,\n",
       " 'mani': 380,\n",
       " 'unnecessari': 381,\n",
       " 'detail': 382,\n",
       " 'bad': 383,\n",
       " 'damag': 384,\n",
       " 'yeah': 385,\n",
       " 'studi': 386,\n",
       " 'nowdeepu': 387,\n",
       " 'snowflak': 388,\n",
       " 'alway': 389,\n",
       " 'symmetr': 390,\n",
       " 'geometri': 391,\n",
       " 'state': 392,\n",
       " 'six': 393,\n",
       " 'arm': 394,\n",
       " 'assert': 395,\n",
       " 'true': 396,\n",
       " 'accord': 397,\n",
       " 'kenneth': 398,\n",
       " 'libbrecht': 399,\n",
       " 'rather': 400,\n",
       " 'unattract': 401,\n",
       " 'irregular': 402,\n",
       " 'crystal': 403,\n",
       " 'far': 404,\n",
       " 'common': 405,\n",
       " 'varieti': 406,\n",
       " 'site': 407,\n",
       " 'get': 408,\n",
       " 'fact': 409,\n",
       " 'still': 410,\n",
       " 'decent': 411,\n",
       " 'number': 412,\n",
       " 'falsiti': 413,\n",
       " 'forgiv': 414,\n",
       " 'signpost': 415,\n",
       " 'septemb': 416,\n",
       " 'singlepag': 417,\n",
       " 'unsubscrib': 418,\n",
       " 'reconsid': 419,\n",
       " 'st': 420,\n",
       " 'paragraph': 421,\n",
       " 'understand': 422,\n",
       " 'reason': 423,\n",
       " 'data': 424,\n",
       " 'necessarili': 425,\n",
       " 'wrong': 426,\n",
       " 'persuad': 427,\n",
       " 'strategi': 428,\n",
       " 'introduc': 429,\n",
       " 'academ': 430,\n",
       " 'honor': 431,\n",
       " 'unhelp': 432,\n",
       " 'approach': 433,\n",
       " 'sit': 434,\n",
       " 'justic': 435,\n",
       " 'similarli': 436,\n",
       " 'enhanc': 437,\n",
       " 'support': 438,\n",
       " 'view': 439,\n",
       " 'invit': 440,\n",
       " 'anyon': 441,\n",
       " 'revisit': 442,\n",
       " 'written': 443,\n",
       " 'pair': 444,\n",
       " 'jurist': 445,\n",
       " 'benjamin': 446,\n",
       " 'cardozo': 447,\n",
       " 'learn': 448,\n",
       " 'hand': 449,\n",
       " 'b': 450,\n",
       " 'john': 451,\n",
       " 'marshal': 452,\n",
       " 'harlan': 453,\n",
       " 'ii': 454,\n",
       " 'becom': 455,\n",
       " 'version': 456,\n",
       " 'either': 457,\n",
       " 'credenti': 458,\n",
       " 'introductori': 459,\n",
       " 'help': 460,\n",
       " 'repeat': 461,\n",
       " 'wri': 462,\n",
       " 'kathleen': 463,\n",
       " 'sullivan': 464,\n",
       " 'stanford': 465,\n",
       " 'law': 466,\n",
       " 'harvard': 467,\n",
       " 'faculti': 468,\n",
       " 'antonin': 469,\n",
       " 'scalia': 470,\n",
       " 'avoid': 471,\n",
       " 'other': 472,\n",
       " 'manag': 473,\n",
       " 'grasp': 474,\n",
       " 'process': 475,\n",
       " 'judg': 476,\n",
       " 'anecdot': 477,\n",
       " 'gentli': 478,\n",
       " 'illustr': 479,\n",
       " 'less': 480,\n",
       " 'humor': 481,\n",
       " 'stronger': 482,\n",
       " 'clarenc': 483,\n",
       " 'thoma': 484,\n",
       " 'mention': 485,\n",
       " 'return': 486,\n",
       " 'degre': 487,\n",
       " 'yale': 488,\n",
       " 'minimum': 489,\n",
       " 'deserv': 490,\n",
       " 'radial': 491,\n",
       " 'symmetri': 492,\n",
       " 'sever': 493,\n",
       " 'extinct': 494,\n",
       " 'lineag': 495,\n",
       " 'echinodermata': 496,\n",
       " 'bilater': 497,\n",
       " 'homostelea': 498,\n",
       " 'asymmetr': 499,\n",
       " 'cothurnocysti': 500,\n",
       " 'stylophora': 501,\n",
       " 'there': 502,\n",
       " 'reconcil': 503,\n",
       " 'knowledg': 504,\n",
       " 'youv': 505,\n",
       " 'done': 506,\n",
       " 'histori': 507,\n",
       " 'archaeolog': 508,\n",
       " 'scan': 509,\n",
       " 'email': 510,\n",
       " 'translat': 511,\n",
       " 'mother': 512,\n",
       " 'child': 513,\n",
       " 'case': 514,\n",
       " 'michael': 515,\n",
       " 'jackson': 516,\n",
       " 'motiv': 517,\n",
       " 'upon': 518,\n",
       " 'charact': 519,\n",
       " 'harshli': 520,\n",
       " 'wacko': 521,\n",
       " 'jacko': 522,\n",
       " 'tell': 523,\n",
       " 'ignor': 524,\n",
       " 'incrimin': 525,\n",
       " 'continu': 526,\n",
       " 'refut': 527,\n",
       " 'bullshit': 528,\n",
       " 'jayjg': 529,\n",
       " 'keep': 530,\n",
       " 'jun': 531,\n",
       " 'ok': 532,\n",
       " 'bit': 533,\n",
       " 'base': 534,\n",
       " 'duck': 535,\n",
       " 'barnstar': 536,\n",
       " 'life': 537,\n",
       " 'us': 538,\n",
       " 'star': 539,\n",
       " 'block': 540,\n",
       " 'expir': 541,\n",
       " 'funni': 542,\n",
       " 'thing': 543,\n",
       " 'uncivil': 544,\n",
       " 'head': 545,\n",
       " 'fight': 546,\n",
       " 'freedom': 547,\n",
       " 'contain': 548,\n",
       " 'prais': 549,\n",
       " 'month': 550,\n",
       " 'ago': 551,\n",
       " 'much': 552,\n",
       " 'abl': 553,\n",
       " 'quickli': 554,\n",
       " 'text': 555,\n",
       " 'hard': 556,\n",
       " 'drive': 557,\n",
       " 'ive': 558,\n",
       " 'updat': 559,\n",
       " 'sound': 560,\n",
       " 'gener': 561,\n",
       " 'interest': 562,\n",
       " 'spent': 563,\n",
       " 'four': 564,\n",
       " 'drum': 565,\n",
       " 'freeli': 566,\n",
       " 'length': 567,\n",
       " 'classic': 568,\n",
       " 'music': 569,\n",
       " 'unfortun': 570,\n",
       " 'attempt': 571,\n",
       " 'fail': 572,\n",
       " 'effect': 573,\n",
       " 'wikiproject': 574,\n",
       " 'wikipediatalkwikiprojectclassicalmusicarchiveneedhelpwikipediatalkwikiprojectmusicarchiveicouldusesomehelpwikipediatalkwikiprojectmusicarchiveraulbotcandthemusiclist': 575,\n",
       " 'given': 576,\n",
       " 'featur': 577,\n",
       " 'digg': 578,\n",
       " 'got': 579,\n",
       " 'opinion': 580,\n",
       " 'impress': 581,\n",
       " 'subpag': 582,\n",
       " 'rfa': 583,\n",
       " 'noseptemb': 584,\n",
       " 'elc': 585,\n",
       " 'surpris': 586,\n",
       " 'left': 587,\n",
       " 'tc': 588,\n",
       " 'straw': 589,\n",
       " 'never': 590,\n",
       " 'claim': 591,\n",
       " 'odonohu': 592,\n",
       " 'posit': 593,\n",
       " 'practition': 594,\n",
       " 'research': 595,\n",
       " 'field': 596,\n",
       " 'dsm': 597,\n",
       " 'exactli': 598,\n",
       " 'quot': 599,\n",
       " 'say': 600,\n",
       " 'agre': 601,\n",
       " 'combat': 602,\n",
       " 'notion': 603,\n",
       " 'absurd': 604,\n",
       " 'part': 605,\n",
       " 'pedophilia': 606,\n",
       " 'sexual': 607,\n",
       " 'orient': 608,\n",
       " 'hold': 609,\n",
       " 'unfair': 610,\n",
       " 'call': 611,\n",
       " 'disord': 612,\n",
       " 'divid': 613,\n",
       " 'end': 614,\n",
       " 'day': 615,\n",
       " 'valu': 616,\n",
       " 'judgment': 617,\n",
       " 'cantor': 618,\n",
       " 'earlier': 619,\n",
       " 'thread': 620,\n",
       " 'scientif': 621,\n",
       " 'judgement': 622,\n",
       " 'choos': 623,\n",
       " 'clearli': 624,\n",
       " 'pretend': 625,\n",
       " 'basi': 626,\n",
       " 'mainland': 627,\n",
       " 'asia': 628,\n",
       " 'lower': 629,\n",
       " 'basin': 630,\n",
       " 'china': 631,\n",
       " 'yangtz': 632,\n",
       " 'river': 633,\n",
       " 'korea': 634,\n",
       " 'fine': 635,\n",
       " 'found': 636,\n",
       " 'citat': 637,\n",
       " 'comprehens': 638,\n",
       " 'dna': 639,\n",
       " 'hammer': 640,\n",
       " 'generar': 641,\n",
       " 'specul': 642,\n",
       " 'yayoi': 643,\n",
       " 'cultur': 644,\n",
       " 'brought': 645,\n",
       " 'japan': 646,\n",
       " 'migrant': 647,\n",
       " 'trace': 648,\n",
       " 'root': 649,\n",
       " 'southeast': 650,\n",
       " 'asiasouth': 651,\n",
       " 'migrat': 652,\n",
       " 'osri': 653,\n",
       " 'gene': 654,\n",
       " 'close': 655,\n",
       " 'haplogroup': 656,\n",
       " 'om': 657,\n",
       " 'reiter': 658,\n",
       " 'propos': 659,\n",
       " 'asian': 660,\n",
       " 'origin': 661,\n",
       " 'definit': 662,\n",
       " 'southern': 663,\n",
       " 'hypothes': 664,\n",
       " 'dispers': 665,\n",
       " 'neolith': 666,\n",
       " 'farmer': 667,\n",
       " 'eventu': 668,\n",
       " 'conclud': 669,\n",
       " 'chromosom': 670,\n",
       " 'descend': 671,\n",
       " 'prehistor': 672,\n",
       " 'southeastern': 673,\n",
       " 'agricultur': 674,\n",
       " 'region': 675,\n",
       " 'global': 676,\n",
       " 'sampl': 677,\n",
       " 'male': 678,\n",
       " 'popul': 679,\n",
       " 'across': 680,\n",
       " 'japanes': 681,\n",
       " 'archipelago': 682,\n",
       " 'pretti': 683,\n",
       " 'everyon': 684,\n",
       " 'warren': 685,\n",
       " 'countysurround': 686,\n",
       " 'glen': 687,\n",
       " 'fall': 688,\n",
       " 'hospit': 689,\n",
       " 'qualifi': 690,\n",
       " 'nativ': 691,\n",
       " 'rachel': 692,\n",
       " 'ray': 693,\n",
       " 'lake': 694,\n",
       " 'luzern': 695,\n",
       " 'preced': 696,\n",
       " 'unsign': 697,\n",
       " 'comment': 698,\n",
       " 'august': 699,\n",
       " 'hi': 700,\n",
       " 'explicit': 701,\n",
       " 'fenian': 702,\n",
       " 'editwar': 703,\n",
       " 'giant': 704,\n",
       " 'causeway': 705,\n",
       " 'terror': 706,\n",
       " 'rurika': 707,\n",
       " 'kasuga': 708,\n",
       " 'place': 709,\n",
       " 'speedili': 710,\n",
       " 'person': 711,\n",
       " 'peopl': 712,\n",
       " 'band': 713,\n",
       " 'club': 714,\n",
       " 'compani': 715,\n",
       " 'web': 716,\n",
       " 'indic': 717,\n",
       " 'guidelin': 718,\n",
       " 'accept': 719,\n",
       " 'contest': 720,\n",
       " 'exist': 721,\n",
       " 'db': 722,\n",
       " 'leav': 723,\n",
       " 'explain': 724,\n",
       " 'hesit': 725,\n",
       " 'confirm': 726,\n",
       " 'biographi': 727,\n",
       " 'feel': 728,\n",
       " 'free': 729,\n",
       " 'lead': 730,\n",
       " 'briefli': 731,\n",
       " 'summar': 732,\n",
       " 'armenia': 733,\n",
       " 'necessari': 734,\n",
       " 'sentenc': 735,\n",
       " 'redund': 736,\n",
       " 'welcom': 737,\n",
       " 'tfd': 738,\n",
       " 'ece': 739,\n",
       " 'respond': 740,\n",
       " 'without': 741,\n",
       " 'respons': 742,\n",
       " 'saw': 743,\n",
       " 'mine': 744,\n",
       " 'tcwpchicagowpfour': 745,\n",
       " 'gay': 746,\n",
       " 'antisemmitian': 747,\n",
       " 'archangel': 748,\n",
       " 'white': 749,\n",
       " 'tiger': 750,\n",
       " 'meow': 751,\n",
       " 'greetingshhh': 752,\n",
       " 'uh': 753,\n",
       " 'two': 754,\n",
       " 'way': 755,\n",
       " 'eras': 756,\n",
       " 'ww': 757,\n",
       " 'holocaust': 758,\n",
       " 'brutal': 759,\n",
       " 'slay': 760,\n",
       " 'jew': 761,\n",
       " 'gaysgypsysslavsanyon': 762,\n",
       " 'antisemitian': 763,\n",
       " 'shave': 764,\n",
       " 'bald': 765,\n",
       " 'skinhead': 766,\n",
       " 'doubt': 767,\n",
       " 'bibl': 768,\n",
       " 'homosexu': 769,\n",
       " 'deadli': 770,\n",
       " 'sin': 771,\n",
       " 'pentagram': 772,\n",
       " 'tatoo': 773,\n",
       " 'forehead': 774,\n",
       " 'satanist': 775,\n",
       " 'mass': 776,\n",
       " 'pal': 777,\n",
       " 'last': 778,\n",
       " 'fuck': 779,\n",
       " 'wont': 780,\n",
       " 'appreci': 781,\n",
       " 'nazi': 782,\n",
       " 'shwain': 783,\n",
       " 'wish': 784,\n",
       " 'anymor': 785,\n",
       " 'bewar': 786,\n",
       " 'dark': 787,\n",
       " 'side': 788,\n",
       " 'filthi': 789,\n",
       " 'ass': 790,\n",
       " 'dri': 791,\n",
       " 'screw': 792,\n",
       " 'domin': 793,\n",
       " 'bow': 794,\n",
       " 'almighti': 795,\n",
       " 'administr': 796,\n",
       " 'play': 797,\n",
       " 'outsidewith': 798,\n",
       " 'mom': 799,\n",
       " 'lisak': 800,\n",
       " 'critic': 801,\n",
       " 'present': 802,\n",
       " 'conform': 803,\n",
       " 'npv': 804,\n",
       " 'rule': 805,\n",
       " 'neutral': 806,\n",
       " 'begin': 807,\n",
       " 'offer': 808,\n",
       " 'polygraph': 809,\n",
       " 'concern': 810,\n",
       " 'result': 811,\n",
       " 'shock': 812,\n",
       " 'complain': 813,\n",
       " 'lie': 814,\n",
       " 'uncov': 815,\n",
       " 'recant': 816,\n",
       " 'perfectli': 817,\n",
       " 'valid': 818,\n",
       " 'truth': 819,\n",
       " 'machin': 820,\n",
       " 'investig': 821,\n",
       " 'kanin': 822,\n",
       " 'followup': 823,\n",
       " 'stori': 824,\n",
       " 'possibl': 825,\n",
       " 'verifi': 826,\n",
       " 'fals': 827,\n",
       " 'happen': 828,\n",
       " 'respect': 829,\n",
       " 'phd': 830,\n",
       " 'baseless': 831,\n",
       " 'wasnt': 832,\n",
       " 'though': 833,\n",
       " 'ammend': 834,\n",
       " 'appropri': 835,\n",
       " 'notabilitysignific': 836,\n",
       " 'lazi': 837,\n",
       " 'goe': 838,\n",
       " 'stalk': 839,\n",
       " 'absolut': 840,\n",
       " 'rubbish': 841,\n",
       " 'serv': 842,\n",
       " 'aggrav': 843,\n",
       " 'assum': 844,\n",
       " 'faith': 845,\n",
       " 'intent': 846,\n",
       " 'seen': 847,\n",
       " 'might': 848,\n",
       " 'ulterior': 849,\n",
       " 'massad': 850,\n",
       " 'ever': 851,\n",
       " 'role': 852,\n",
       " 'parti': 853,\n",
       " 'disagr': 854,\n",
       " 'rate': 855,\n",
       " 'conflict': 856,\n",
       " 'thu': 857,\n",
       " 'extend': 858,\n",
       " 'toward': 859,\n",
       " 'spuriou': 860,\n",
       " 'unfound': 861,\n",
       " 'chatspi': 862,\n",
       " 'jmabel': 863,\n",
       " 'predomin': 864,\n",
       " 'scholari': 865,\n",
       " 'consensu': 866,\n",
       " 'allegedli': 867,\n",
       " 'despit': 868,\n",
       " 'rhetor': 869,\n",
       " 'fascism': 870,\n",
       " 'function': 871,\n",
       " 'rightw': 872,\n",
       " 'forc': 873,\n",
       " 'awar': 874,\n",
       " 'own': 875,\n",
       " 'numer': 876,\n",
       " 'book': 877,\n",
       " 'develop': 878,\n",
       " 'scholar': 879,\n",
       " 'manner': 880,\n",
       " 'bia': 881,\n",
       " 'roger': 882,\n",
       " 'griffin': 883,\n",
       " 'hamish': 884,\n",
       " 'mcdonald': 885,\n",
       " 'eatwel': 886,\n",
       " 'zeev': 887,\n",
       " 'sternhel': 888,\n",
       " 'recongis': 889,\n",
       " 'show': 890,\n",
       " 'dissent': 891,\n",
       " 'absout': 892,\n",
       " 'leftist': 893,\n",
       " 'connect': 894,\n",
       " 'radic': 895,\n",
       " 'right': 896,\n",
       " 'system': 897,\n",
       " 'street': 898,\n",
       " 'socialist': 899,\n",
       " 'put': 900,\n",
       " 'distanc': 901,\n",
       " 'movement': 902,\n",
       " 'cours': 903,\n",
       " 'educ': 904,\n",
       " 'foremost': 905,\n",
       " 'expert': 906,\n",
       " 'former': 907,\n",
       " 'member': 908,\n",
       " 'communist': 909,\n",
       " 'itali': 910,\n",
       " 'renzo': 911,\n",
       " 'de': 912,\n",
       " 'felic': 913,\n",
       " 'cover': 914,\n",
       " 'wrote': 915,\n",
       " 'seven': 916,\n",
       " 'volum': 917,\n",
       " 'piec': 918,\n",
       " 'mussolini': 919,\n",
       " 'bi': 920,\n",
       " 'bottom': 921,\n",
       " 'promot': 922,\n",
       " 'skyhook': 923,\n",
       " 'concept': 924,\n",
       " 'cost': 925,\n",
       " 'competit': 926,\n",
       " 'realist': 927,\n",
       " 'thought': 928,\n",
       " 'space': 929,\n",
       " 'elev': 930,\n",
       " 'rotat': 931,\n",
       " 'tether': 932,\n",
       " 'deem': 933,\n",
       " 'engineeringli': 934,\n",
       " 'feasibl': 935,\n",
       " 'avail': 936,\n",
       " 'materi': 937,\n",
       " 'statement': 938,\n",
       " 'ref': 939,\n",
       " 'alon': 940,\n",
       " 'exceed': 941,\n",
       " 'payload': 942,\n",
       " 'particular': 943,\n",
       " 'scenario': 944,\n",
       " 'although': 945,\n",
       " 'applic': 946,\n",
       " 'near': 947,\n",
       " 'futur': 948,\n",
       " 'higher': 949,\n",
       " 'tensil': 950,\n",
       " 'strength': 951,\n",
       " 'oper': 952,\n",
       " 'temperatur': 953,\n",
       " 'shall': 954,\n",
       " 'commerci': 955,\n",
       " 'suffic': 956,\n",
       " 'hastol': 957,\n",
       " 'primari': 958,\n",
       " 'messag': 959,\n",
       " 'reader': 960,\n",
       " 'magic': 961,\n",
       " 'buckminsterfullercarbonnanotub': 962,\n",
       " 'facil': 963,\n",
       " 'misread': 964,\n",
       " 'upper': 965,\n",
       " 'limit': 966,\n",
       " 'problem': 967,\n",
       " 'built': 968,\n",
       " 'reinforc': 969,\n",
       " 'conclus': 970,\n",
       " 'report': 971,\n",
       " 'fundament': 972,\n",
       " 'phase': 973,\n",
       " 'effort': 974,\n",
       " 'technic': 975,\n",
       " 'evalu': 976,\n",
       " 'altern': 977,\n",
       " 'configur': 978,\n",
       " 'allow': 979,\n",
       " 'hyperson': 980,\n",
       " 'airbreath': 981,\n",
       " 'vehicl': 982,\n",
       " 'technolog': 983,\n",
       " 'combin': 984,\n",
       " 'orbit': 985,\n",
       " 'spin': 986,\n",
       " 'provid': 987,\n",
       " 'method': 988,\n",
       " 'move': 989,\n",
       " 'surfac': 990,\n",
       " 'earth': 991,\n",
       " 'architectur': 992,\n",
       " 'design': 993,\n",
       " 'solut': 994,\n",
       " 'nearterm': 995,\n",
       " 'expect': 996,\n",
       " 'prove': 997,\n",
       " 'complet': 998,\n",
       " 'reusabl': 999,\n",
       " 'potenti': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T23:03:35.378871Z",
     "start_time": "2025-04-02T23:03:35.230671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Total vocabulary size:\", len(word2id))\n",
    "print(\"Example word mappings:\", list(word2id.items())[:10])"
   ],
   "id": "880850e1a9b281b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 612571\n",
      "Example word mappings: [('explan', 1), ('edit', 2), ('made', 3), ('usernam', 4), ('hardcor', 5), ('metallica', 6), ('fan', 7), ('revert', 8), ('werent', 9), ('vandal', 10)]\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T23:05:08.604475Z",
     "start_time": "2025-04-02T23:04:45.173864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert tokenized text to numerical sequences\n",
    "df['sequence'] = df['tokenized_comment'].apply(lambda tokens: [word2id[word] for word in tokens])\n",
    "\n",
    "# Show example\n",
    "print(df[['tokenized_comment', 'sequence']].head())\n"
   ],
   "id": "a369a7f4fc797ac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_comment  \\\n",
      "0  [explan, edit, made, usernam, hardcor, metalli...   \n",
      "1  [daww, match, background, colour, im, seemingl...   \n",
      "2  [hey, man, im, realli, tri, edit, war, guy, co...   \n",
      "3  [cant, make, real, suggest, improv, wonder, se...   \n",
      "4             [sir, hero, chanc, rememb, page, that]   \n",
      "\n",
      "                                            sequence  \n",
      "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
      "1  [27, 28, 29, 30, 25, 31, 32, 33, 22, 34, 35, 3...  \n",
      "2  [38, 39, 25, 40, 41, 2, 42, 43, 44, 20, 45, 46...  \n",
      "3  [53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 6...  \n",
      "4                           [91, 92, 93, 94, 23, 95]  \n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T23:22:49.702286Z",
     "start_time": "2025-04-02T23:21:30.683900Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_csv(output_file, index=False)\n",
   "id": "81044eaa10d6e4ae",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:22:01.242624Z",
     "start_time": "2025-04-03T16:21:41.914817Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(output_file)",
   "id": "22cf849cd74eeb4e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:27:56.291995Z",
     "start_time": "2025-04-03T16:27:49.835924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_word_count = max(df['comment_text'].apply(lambda x: len(str(x).split())))\n",
    "print(max_word_count)\n"
   ],
   "id": "131a87840b8934a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:39:36.337451Z",
     "start_time": "2025-04-03T16:39:30.559649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_index = df['comment_text'].apply(lambda x: len(str(x).split())).idxmax()\n",
    "print(df.loc[max_index, 'comment_text'])  # Prints the comment with max words\n"
   ],
   "id": "5408febb8f762233",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " all of those statements need to be referenced  leaving an article unreferenced in every section opens the door for original research and the introduction for factually inaccurate information  to point out one thing ill bring up consolidated appropriations act 2012  first it has a number of tags on it the creation of articles that need to be tagged should be reason number 1 to remove the flag  second the only referencelink provided on the article is this one while the information is from various subpages of that  it appears that the entirety of the article is found at this deeper link  its possible that some came from this as well i cant tell  ill also note that the style of the summary it was taken from was never meant to be encyclopedic so the article is riddled with grammatical errors mostly incomplete sentences  on creation the opening read consolidated appropriations act 2012 hour 2055 is a bill passed to the 112th united states congress  ive never heard this usage to rather than by and assume it is incorrect another editor changed it the lead is also missing an important determiner  in any case you can take a look at vesey \n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-04-02T23:26:22.338541300Z",
     "start_time": "2025-04-02T23:22:55.589685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequence(seq, max_length):\n",
    "    return seq + [0] * (max_length - len(seq))  # Pad with 0\n",
    "\n",
    "df['padded_sequence'] = df['sequence'].apply(lambda seq: pad_sequence(seq, max_length))\n",
    "\n",
    "# Convert to NumPy array for model training\n",
    "X = np.array(df['padded_sequence'].tolist())\n",
    "\n",
    "print(\"Final shape of input data:\", X.shape)  # Should be (num_samples, max_length)\n"
   ],
   "id": "5f9212cbb923970f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:39:28.873889Z",
     "start_time": "2025-04-03T16:39:28.288656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find the index of the longest comment\n",
    "longest_comment_idx = df['sequence'].apply(len).idxmax()\n",
    "\n",
    "# Get the longest comment and its length\n",
    "longest_comment = df.loc[longest_comment_idx, 'comment_text']\n",
    "longest_comment_length = len(df.loc[longest_comment_idx, 'sequence'])\n",
    "\n",
    "print(\"Longest Comment:\", longest_comment)\n",
    "print(\"Length of Longest Comment (in tokens):\", df.loc[longest_comment_idx, 'sequence'])\n"
   ],
   "id": "82c1c8f7938ac386",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Comment: poooooooooooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooop\n",
      "Length of Longest Comment (in tokens): [235151, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235152, 235153]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T16:39:30.541683Z",
     "start_time": "2025-04-03T16:39:28.907889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the lengths of the comments and store them as a list of tuples (length, comment)\n",
    "comment_lengths = [(len(seq), comment) for seq, comment in zip(df['sequence'], df['comment_text'])]\n",
    "\n",
    "# Sort by length in descending order and get the top 10 longest comments\n",
    "top_10_longest_comments = sorted(comment_lengths, key=lambda x: x[0], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 longest comments and their lengths\n",
    "for length, comment in top_10_longest_comments:\n",
    "    print(f\"Length: {length}, Comment: {comment[:200]}...\")  # Print the first 200 characters of the comment for readability\n"
   ],
   "id": "7b3073f790386801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1472, Comment: poooooooooooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ooooooooo ...\n",
      "Length: 1424, Comment: mis horny horny  horny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny hornyhorny h...\n",
      "Length: 1303, Comment: problem    beschwerdefhrer benutzerwhiggsgerm    beteiligte administratoren benutzerinitti und benutzerjkb    missbruchlich eingesetzte funktion benutzersperre    erluterung  ohne bzw mit falscher und...\n",
      "Length: 1267, Comment:  lied der deutschen  nazilied     lieber prohibitonions    schade da du in der englischen ausgabe der wikipedia unverantwortliches unwissen verbreitest die nationalhymne war wie durch den schriftverke...\n",
      "Length: 1254, Comment:  o lucilio  um filho da puta     lucilio filho da puta lucilio filho da puta lucilio filho da puta lucilio filho da puta lucilio filho da puta lucilio filho da puta lucilio filho da puta lucilio filho...\n",
      "Length: 1247, Comment: browns gas    brownsches gas    wasser h2o kennt neben eis fluessigem wasser und dampf noch einen 4 zustand das haben physiker jetzt entdeckt     bei bestimmten experimenten mit wasserstoffplasma kann...\n",
      "Length: 1245, Comment: epic fail epic fail epic fail epic failepic fail epic failepic fail epic failepic fail epic failepic fail epic failepic fail epic failepic fail epic failepic fail epic failepic fail epic failepic fail...\n",
      "Length: 1239, Comment:  harapan anak aceh             galak teuh tamat pasport dro teuhpasport acehgalak teuh tajaweb dari aceh meunyeu teukeudi na ureung tanyong dari panemale hate teuh tajaweb dro teuh dari indonesia sawe...\n",
      "Length: 1233, Comment: ptptpthtphthpthhhhhhh fartfartfartfartfartfartfartfartfartfartfartfartfartfartfartfartfartfartfartfartfartfart nipple nipple nipple nipple nipple nipple nipple nipple nipple nipple nipple nipple nippl...\n",
      "Length: 1186, Comment:   from metalanguage codesconflicts below is a list of conflicts between 2 letter language codes and country codes  a conflict occurs when a country uses the same code as a language it does not activel...\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b2652fcaee3458ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
